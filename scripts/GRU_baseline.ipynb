{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, GRU, RepeatVector, TimeDistributed, Dense\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "PIVOTED_FILE = \"pivoted_data.parquet\"\n",
    "ANOMALY_WINDOWS_FILE = \"anomaly_windows.csv\"\n",
    "OUT_DIR = os.path.join(\"userData\", \"pivoted_paper_baseline\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "SEQ_LEN = 1\n",
    "ENCODER_UNITS = [16] * 7      \n",
    "DECODER_UNITS = [16] * 7     \n",
    "GRU_ACT = \"relu\"\n",
    "USE_BOTTLENECK = True\n",
    "BOTTLENECK_SIZE = 14\n",
    "\n",
    "W_LONG, W_SHORT = 30, 2    \n",
    "PAPER_THR = 0.9973          \n",
    "ROUND_RES = \"5T\"             \n",
    "EVENT_GAP_MINUTES = 15      \n",
    "PCT_PROB = 0.05               \n",
    "\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_smart(path, time_col=\"interval_start\"):\n",
    "    df = pd.read_parquet(path)\n",
    "    if time_col in df.columns and not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
    "        df = df.sort_values(time_col).set_index(time_col)\n",
    "    if isinstance(df.index, pd.DatetimeIndex) and df.index.year.max() < 1990:\n",
    "        ns = df.index.view(\"int64\")\n",
    "        maxv = float(ns.max())\n",
    "        if maxv < 1e12:\n",
    "            scale = 1_000_000_000\n",
    "        elif maxv < 1e15:\n",
    "            scale = 1_000_000\n",
    "        elif maxv < 1e18:\n",
    "            scale = 1_000\n",
    "        else:\n",
    "            scale = 1\n",
    "        df = df.copy()\n",
    "        df.index = pd.to_datetime(ns * scale, unit=\"ns\")\n",
    "    return df\n",
    "\n",
    "def add_time_features(index):\n",
    "    t = pd.Series(index.view(\"int64\") // 10**9, index=index)\n",
    "    day = 24 * 3600\n",
    "    week = 7 * day\n",
    "    return pd.DataFrame({\n",
    "        \"sin_day\": np.sin(2 * np.pi * (t % day) / day),\n",
    "        \"cos_day\": np.cos(2 * np.pi * (t % day) / day),\n",
    "        \"sin_week\": np.sin(2 * np.pi * (t % week) / week),\n",
    "        \"cos_week\": np.cos(2 * np.pi * (t % week) / week),\n",
    "    }, index=index).astype(np.float32)\n",
    "\n",
    "def make_sequences(df_values, L=1):\n",
    "    arr = df_values.values\n",
    "    if L == 1:\n",
    "        return arr[:, None, :]\n",
    "    n = len(arr) - L + 1\n",
    "    return np.stack([arr[i:i+L] for i in range(n)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAB_PROFILES = {\n",
    "    \"standard\": {\"TP\": 1.0, \"FN\": -1.0, \"FP\": -0.11, \"TN\": 0.0},\n",
    "    \"low_fp\":  {\"TP\": 1.0, \"FN\": -1.0, \"FP\": -0.22, \"TN\": 0.0},\n",
    "    \"low_fn\": {\"TP\": 1.0, \"FN\": -2.0, \"FP\": -0.11, \"TN\": 0.0}\n",
    "}\n",
    "\n",
    "def scaled_sigmoid(x, lo, hi, slope=5.0):\n",
    "    v = 1.0 / (1.0 + np.exp(-slope * x))\n",
    "    return lo + (hi - lo) * v\n",
    "\n",
    "def tp_score(time_hit, start, end, tp_weight):\n",
    "    time_hit = pd.Timestamp(time_hit)\n",
    "    start = pd.Timestamp(start)\n",
    "    end = pd.Timestamp(end)\n",
    "    L = (end - start).total_seconds()\n",
    "    if L <= 0:\n",
    "        return float(tp_weight)\n",
    "    if time_hit <= start + pd.Timedelta(microseconds=1):\n",
    "        return float(tp_weight)\n",
    "    r = (time_hit - start).total_seconds() / L\n",
    "    r = np.clip(r, 0.0, 1.0)\n",
    "    x = 1.0 - 2.0 * r\n",
    "    return float(scaled_sigmoid(x, 0.0, float(tp_weight)))\n",
    "\n",
    "def fp_penalty(time_hit, windows, fp_weight):\n",
    "    if not windows:\n",
    "        return float(fp_weight)\n",
    "    dists = []\n",
    "    lens = []\n",
    "    for (s, e) in windows:\n",
    "        if s <= time_hit <= e:\n",
    "            dists.append(0.0)\n",
    "        elif time_hit < s:\n",
    "            dists.append((s - time_hit).total_seconds())\n",
    "        else:\n",
    "            dists.append((time_hit - e).total_seconds())\n",
    "        lens.append((e - s).total_seconds())\n",
    "    d = float(min(dists))\n",
    "    mean_len = float(np.mean(lens)) if len(lens) > 0 else 60.0\n",
    "    z = d / max(mean_len, 1.0)\n",
    "    val = scaled_sigmoid(z, float(fp_weight), 0.0)\n",
    "    return float(min(0.0, val))\n",
    "\n",
    "def read_gt_windows(csv_path, start_hint=\"start\", end_hint=\"end\"):\n",
    "    gt = pd.read_csv(csv_path)\n",
    "    s_cols = [c for c in gt.columns if start_hint in c.lower()]\n",
    "    e_cols = [c for c in gt.columns if end_hint in c.lower()]\n",
    "    if not s_cols or not e_cols:\n",
    "        raise ValueError(\"Could not find start/end columns in anomaly_windows.csv\")\n",
    "    s_col = s_cols[0]\n",
    "    e_col = e_cols[0]\n",
    "    df = pd.DataFrame({\n",
    "        \"anomaly_start\": pd.to_datetime(gt[s_col].astype(str).str.replace(r'([+-]\\d{2}:?\\d{2}|Z)$','', regex=True), errors=\"coerce\"),\n",
    "        \"anomaly_end\":   pd.to_datetime(gt[e_col].astype(str).str.replace(r'([+-]\\d{2}:?\\d{2}|Z)$','', regex=True), errors=\"coerce\")\n",
    "    }).dropna().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def mask_to_event_times(pred_mask, timestamps, event_gap_minutes=15, pct_prob=0.0):\n",
    "    timestamps = np.asarray(timestamps)\n",
    "    pm = np.asarray(pred_mask).astype(bool)\n",
    "    if pct_prob > 0.0:\n",
    "        cutoff = timestamps[0] + (timestamps[-1] - timestamps[0]) * pct_prob\n",
    "        pm = pm & (timestamps >= cutoff)\n",
    "    if not pm.any():\n",
    "        return []\n",
    "    shifted = np.concatenate(([False], pm[:-1]))\n",
    "    edges = np.where(pm & (~shifted))[0]\n",
    "    if len(edges) == 0:\n",
    "        edges = np.array([np.where(pm)[0][0]])\n",
    "    events = [pd.to_datetime(timestamps[edges[0]])]\n",
    "    gap = pd.Timedelta(minutes=event_gap_minutes)\n",
    "    for idx in edges[1:]:\n",
    "        t = pd.to_datetime(timestamps[idx])\n",
    "        if (t - events[-1]) >= gap:\n",
    "            events.append(t)\n",
    "    return events\n",
    "\n",
    "def nab_score_from_mask(pred_mask, gt_windows_df, timestamps, profile=\"standard\", event_gap_minutes=15, pct_prob=0.0):\n",
    "    if profile not in NAB_PROFILES:\n",
    "        raise ValueError(\"Unknown NAB profile: \" + str(profile))\n",
    "    prof = NAB_PROFILES[profile]\n",
    "    TPw, FNw, FPw = prof[\"TP\"], prof[\"FN\"], prof[\"FP\"]\n",
    "    wins = [(pd.Timestamp(r[\"anomaly_start\"]), pd.Timestamp(r[\"anomaly_end\"])) for _, r in gt_windows_df.iterrows()]\n",
    "    events = mask_to_event_times(pred_mask, timestamps, event_gap_minutes, pct_prob)\n",
    "    score = 0.0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    used = [False] * len(wins)\n",
    "    for ev in events:\n",
    "        hit = -1\n",
    "        for i, (s, e) in enumerate(wins):\n",
    "            if (not used[i]) and (s <= ev <= e):\n",
    "                hit = i\n",
    "                break\n",
    "        if hit >= 0:\n",
    "            score += tp_score(ev, wins[hit][0], wins[hit][1], TPw)\n",
    "            used[hit] = True\n",
    "            TP += 1\n",
    "        else:\n",
    "            score += fp_penalty(ev, wins, FPw)\n",
    "            FP += 1\n",
    "    FN = sum(not u for u in used)\n",
    "    score += FNw * FN\n",
    "    perfect = TPw * len(wins)\n",
    "    null = FNw * len(wins)\n",
    "    if np.isclose(perfect, null):\n",
    "        normalized = 0.0\n",
    "    else:\n",
    "        normalized = 100.0 * (score - null) / (perfect - null)\n",
    "    return float(normalized), {\"TP\": int(TP), \"FN\": int(FN), \"FP\": int(FP)}\n",
    "\n",
    "def likelihood_scores_from_s(err, W=30, Wp=2):\n",
    "    T = len(err)\n",
    "    L = np.zeros(T)\n",
    "    eps = 1e-8\n",
    "    for t in range(T):\n",
    "        a = max(0, t - W + 1)\n",
    "        b = t + 1\n",
    "        a2 = max(0, t - Wp + 1)\n",
    "        long_window = err[a:b]\n",
    "        short_window = err[a2:b]\n",
    "        mu = long_window.mean()\n",
    "        std = long_window.std(ddof=1) if (b - a) > 1 else eps\n",
    "        mu_s = short_window.mean()\n",
    "        z = (mu_s - mu) / (std + eps)\n",
    "        L[t] = norm.cdf(z)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: pivoted_data.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data from:\", PIVOTED_FILE)\n",
    "df = read_parquet_smart(PIVOTED_FILE, time_col=\"interval_start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected feature columns: 2406\n"
     ]
    }
   ],
   "source": [
    "include_patterns = [r\"(5xx|_5\\d\\d_)\", r\"_count$\"]\n",
    "compiled = [re.compile(p, re.I) for p in include_patterns]\n",
    "feature_cols = [c for c in df.columns if all(p.search(c) for p in compiled)]\n",
    "print(\"Selected feature columns:\", len(feature_cols))\n",
    "if len(feature_cols) == 0:\n",
    "    raise ValueError(\"No features matched the selection patterns.\")\n",
    "\n",
    "five_xx_cols = [c for c in df.columns if re.search(r\"5xx|_5\\d\\d_\", c, re.I) and c.endswith(\"_count\")]\n",
    "df[\"sum_5xx_count\"] = df[five_xx_cols].fillna(0).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feature_cols].fillna(0).astype(np.float32)\n",
    "X = pd.concat([X, add_time_features(X.index)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 10074 Test rows: 26487\n"
     ]
    }
   ],
   "source": [
    "TRAIN_START, TRAIN_END = pd.Timestamp(\"2024-01-26\"), pd.Timestamp(\"2024-02-29 23:59:59\")\n",
    "TEST_START, TEST_END = pd.Timestamp(\"2024-03-01\"), pd.Timestamp(\"2024-05-31 23:59:59\")\n",
    "\n",
    "train_df = X[(X.index >= TRAIN_START) & (X.index <= TRAIN_END)].copy()\n",
    "test_df  = X[(X.index >= TEST_START) & (X.index <= TEST_END)].copy()\n",
    "print(\"Train rows:\", len(train_df), \"Test rows:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT windows total: 25 GT inside test: 18\n"
     ]
    }
   ],
   "source": [
    "gt_all = read_gt_windows(ANOMALY_WINDOWS_FILE)\n",
    "gt_in_test = gt_all[(gt_all[\"anomaly_end\"] >= test_df.index[0]) & (gt_all[\"anomaly_start\"] <= test_df.index[-1])].reset_index(drop=True)\n",
    "print(\"GT windows total:\", len(gt_all), \"GT inside test:\", len(gt_in_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows removed from training due to masking GT windows: 100\n"
     ]
    }
   ],
   "source": [
    "mask_train_anomalies = True\n",
    "if mask_train_anomalies:\n",
    "    train_idx = train_df.index\n",
    "    keep_mask = np.ones(len(train_idx), dtype=bool)\n",
    "    for s, e in gt_all.itertuples(index=False):\n",
    "        keep_mask &= ~((train_idx >= s) & (train_idx <= e))\n",
    "    removed = len(train_df) - keep_mask.sum()\n",
    "    train_df = train_df.loc[keep_mask]\n",
    "    print(\"Rows removed from training due to masking GT windows:\", removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train seq shape: (9974, 1, 2410) Test seq shape: (26487, 1, 2410)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(train_df), index=train_df.index, columns=train_df.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(test_df), index=test_df.index, columns=test_df.columns)\n",
    "\n",
    "Xtr_seq = make_sequences(X_train_scaled, SEQ_LEN)\n",
    "Xte_seq = make_sequences(X_test_scaled, SEQ_LEN)\n",
    "print(\"Train seq shape:\", Xtr_seq.shape, \"Test seq shape:\", Xte_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model summary:\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 1, 2410)]         0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 1, 16)             116544    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 1, 16)             1632      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 1, 16)             1632      \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 1, 16)             1632      \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 1, 16)             1632      \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 1, 16)             1632      \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "bottleneck (Dense)           (None, 14)                238       \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 1, 14)             0         \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 1, 16)             1536      \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (None, 1, 16)             1632      \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, 1, 16)             1632      \n",
      "_________________________________________________________________\n",
      "gru_10 (GRU)                 (None, 1, 16)             1632      \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (None, 1, 16)             1632      \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (None, 1, 16)             1632      \n",
      "_________________________________________________________________\n",
      "gru_13 (GRU)                 (None, 1, 16)             1632      \n",
      "_________________________________________________________________\n",
      "reconstruction (TimeDistribu (None, 1, 2410)           40970     \n",
      "=================================================================\n",
      "Total params: 178,872\n",
      "Trainable params: 178,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_dim = Xtr_seq.shape[-1]\n",
    "inp = Input(shape=(SEQ_LEN, input_dim), name=\"input\")\n",
    "\n",
    "x = inp\n",
    "\n",
    "for u in ENCODER_UNITS[:-1]:\n",
    "    x = GRU(u, return_sequences=True, activation=GRU_ACT)(x)\n",
    "\n",
    "x = GRU(ENCODER_UNITS[-1], return_sequences=False, activation=GRU_ACT)(x)\n",
    "\n",
    "if USE_BOTTLENECK:\n",
    "    x = Dense(BOTTLENECK_SIZE, activation=\"relu\", name=\"bottleneck\")(x)\n",
    "x = RepeatVector(SEQ_LEN)(x)\n",
    "for u in DECODER_UNITS:\n",
    "    x = GRU(u, return_sequences=True, activation=GRU_ACT)(x)\n",
    "\n",
    "out = TimeDistributed(Dense(input_dim), name=\"reconstruction\")(x)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(LR), loss=\"mse\")\n",
    "print(\"\\nModel summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8976 samples, validate on 998 samples\n",
      "Epoch 1/60\n",
      "8976/8976 [==============================] - 35s 4ms/sample - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 2/60\n",
      "8976/8976 [==============================] - 2s 271us/sample - loss: 0.0012 - val_loss: 9.8035e-04\n",
      "Epoch 3/60\n",
      "8976/8976 [==============================] - 2s 271us/sample - loss: 0.0012 - val_loss: 9.1846e-04\n",
      "Epoch 4/60\n",
      "8976/8976 [==============================] - 2s 271us/sample - loss: 0.0011 - val_loss: 8.4708e-04\n",
      "Epoch 5/60\n",
      "8976/8976 [==============================] - 2s 272us/sample - loss: 9.7724e-04 - val_loss: 7.7162e-04\n",
      "Epoch 6/60\n",
      "8976/8976 [==============================] - 2s 277us/sample - loss: 8.8630e-04 - val_loss: 6.9754e-04\n",
      "Epoch 7/60\n",
      "8976/8976 [==============================] - 2s 273us/sample - loss: 8.0963e-04 - val_loss: 6.4046e-04\n",
      "Epoch 8/60\n",
      "8976/8976 [==============================] - 2s 265us/sample - loss: 7.6537e-04 - val_loss: 6.1952e-04\n",
      "Epoch 9/60\n",
      "8976/8976 [==============================] - 2s 265us/sample - loss: 7.5477e-04 - val_loss: 6.1693e-04\n",
      "Epoch 10/60\n",
      "8976/8976 [==============================] - 2s 265us/sample - loss: 7.5393e-04 - val_loss: 6.2039e-04\n",
      "Epoch 11/60\n",
      "8976/8976 [==============================] - 3s 285us/sample - loss: 7.5387e-04 - val_loss: 6.1962e-04\n",
      "Epoch 12/60\n",
      "8976/8976 [==============================] - 2s 275us/sample - loss: 7.5384e-04 - val_loss: 6.2027e-04\n",
      "Epoch 13/60\n",
      "8976/8976 [==============================] - 2s 269us/sample - loss: 7.5380e-04 - val_loss: 6.2161e-04\n",
      "Epoch 14/60\n",
      "8976/8976 [==============================] - 2s 271us/sample - loss: 7.5383e-04 - val_loss: 6.2184e-04\n",
      "Epoch 15/60\n",
      "8976/8976 [==============================] - 2s 269us/sample - loss: 7.5380e-04 - val_loss: 6.2131e-04\n",
      "Epoch 16/60\n",
      "8976/8976 [==============================] - 2s 278us/sample - loss: 7.5379e-04 - val_loss: 6.2036e-04\n",
      "Epoch 17/60\n",
      "8976/8976 [==============================] - 2s 274us/sample - loss: 7.5381e-04 - val_loss: 6.1989e-04\n",
      "Epoch 18/60\n",
      "8976/8976 [==============================] - 2s 265us/sample - loss: 7.5379e-04 - val_loss: 6.2195e-04\n",
      "Epoch 19/60\n",
      "8976/8976 [==============================] - 2s 264us/sample - loss: 7.5381e-04 - val_loss: 6.2097e-04\n",
      "Epoch 20/60\n",
      "8976/8976 [==============================] - 2s 265us/sample - loss: 7.5383e-04 - val_loss: 6.2100e-04\n",
      "Epoch 21/60\n",
      "8976/8976 [==============================] - 2s 272us/sample - loss: 7.5381e-04 - val_loss: 6.2257e-04\n",
      "Epoch 22/60\n",
      "8976/8976 [==============================] - 2s 277us/sample - loss: 7.5378e-04 - val_loss: 6.2058e-04\n",
      "Epoch 23/60\n",
      "8976/8976 [==============================] - 2s 270us/sample - loss: 7.5387e-04 - val_loss: 6.2198e-04\n",
      "Epoch 24/60\n",
      "8976/8976 [==============================] - 2s 264us/sample - loss: 7.5378e-04 - val_loss: 6.2144e-04\n",
      "Epoch 25/60\n",
      "8976/8976 [==============================] - 2s 265us/sample - loss: 7.5380e-04 - val_loss: 6.2137e-04\n",
      "Epoch 26/60\n",
      "8976/8976 [==============================] - 2s 268us/sample - loss: 7.5379e-04 - val_loss: 6.2049e-04\n",
      "Epoch 27/60\n",
      "8976/8976 [==============================] - 2s 278us/sample - loss: 7.5381e-04 - val_loss: 6.1998e-04\n",
      "Epoch 28/60\n",
      "8976/8976 [==============================] - 2s 278us/sample - loss: 7.5381e-04 - val_loss: 6.2180e-04\n",
      "Epoch 29/60\n",
      "8976/8976 [==============================] - 2s 270us/sample - loss: 7.5377e-04 - val_loss: 6.2062e-04\n",
      "Epoch 30/60\n",
      "8976/8976 [==============================] - 2s 267us/sample - loss: 7.5382e-04 - val_loss: 6.2044e-04\n",
      "Epoch 31/60\n",
      "8976/8976 [==============================] - 2s 269us/sample - loss: 7.5382e-04 - val_loss: 6.2167e-04\n",
      "Epoch 32/60\n",
      "8976/8976 [==============================] - 3s 279us/sample - loss: 7.5379e-04 - val_loss: 6.2127e-04\n",
      "Epoch 33/60\n",
      "8976/8976 [==============================] - 2s 276us/sample - loss: 7.5382e-04 - val_loss: 6.2213e-04\n",
      "Epoch 34/60\n",
      "8976/8976 [==============================] - 2s 265us/sample - loss: 7.5380e-04 - val_loss: 6.2136e-04\n",
      "Epoch 35/60\n",
      "8976/8976 [==============================] - 2s 268us/sample - loss: 7.5381e-04 - val_loss: 6.1950e-04\n",
      "Epoch 36/60\n",
      "8976/8976 [==============================] - 2s 268us/sample - loss: 7.5383e-04 - val_loss: 6.2066e-04\n",
      "Epoch 37/60\n",
      "8976/8976 [==============================] - 2s 275us/sample - loss: 7.5381e-04 - val_loss: 6.2086e-04\n",
      "Epoch 38/60\n",
      "8976/8976 [==============================] - 2s 275us/sample - loss: 7.5380e-04 - val_loss: 6.1980e-04\n",
      "Epoch 39/60\n",
      "8976/8976 [==============================] - 2s 265us/sample - loss: 7.5378e-04 - val_loss: 6.1930e-04\n",
      "Epoch 40/60\n",
      "8976/8976 [==============================] - 2s 266us/sample - loss: 7.5378e-04 - val_loss: 6.2126e-04\n",
      "Epoch 41/60\n",
      "8976/8976 [==============================] - 2s 262us/sample - loss: 7.5381e-04 - val_loss: 6.2005e-04\n",
      "Epoch 42/60\n",
      "8976/8976 [==============================] - 2s 269us/sample - loss: 7.5379e-04 - val_loss: 6.2073e-04\n",
      "Epoch 43/60\n",
      "8976/8976 [==============================] - 2s 274us/sample - loss: 7.5381e-04 - val_loss: 6.2207e-04\n",
      "Epoch 44/60\n",
      "8976/8976 [==============================] - 2s 272us/sample - loss: 7.5379e-04 - val_loss: 6.2061e-04\n",
      "Epoch 45/60\n",
      "8976/8976 [==============================] - 2s 267us/sample - loss: 7.5381e-04 - val_loss: 6.2241e-04\n",
      "Epoch 46/60\n",
      "8976/8976 [==============================] - 2s 267us/sample - loss: 7.5380e-04 - val_loss: 6.2175e-04\n",
      "Epoch 47/60\n",
      "8976/8976 [==============================] - 2s 259us/sample - loss: 7.5378e-04 - val_loss: 6.2027e-04\n",
      "Epoch 48/60\n",
      "8976/8976 [==============================] - 2s 271us/sample - loss: 7.5379e-04 - val_loss: 6.2124e-04\n",
      "Epoch 49/60\n",
      "8976/8976 [==============================] - 2s 275us/sample - loss: 7.5380e-04 - val_loss: 6.2135e-04\n",
      "Epoch 50/60\n",
      "8976/8976 [==============================] - 2s 268us/sample - loss: 7.5380e-04 - val_loss: 6.2268e-04\n",
      "Epoch 51/60\n",
      "8976/8976 [==============================] - 2s 268us/sample - loss: 7.5381e-04 - val_loss: 6.2106e-04\n",
      "Epoch 52/60\n",
      "8976/8976 [==============================] - 2s 268us/sample - loss: 7.5381e-04 - val_loss: 6.2234e-04\n",
      "Epoch 53/60\n",
      "8976/8976 [==============================] - 2s 270us/sample - loss: 7.5382e-04 - val_loss: 6.2142e-04\n",
      "Epoch 54/60\n",
      "8976/8976 [==============================] - 2s 277us/sample - loss: 7.5383e-04 - val_loss: 6.2084e-04\n",
      "Epoch 55/60\n",
      "8976/8976 [==============================] - 2s 273us/sample - loss: 7.5381e-04 - val_loss: 6.2019e-04\n",
      "Epoch 56/60\n",
      "8976/8976 [==============================] - 2s 268us/sample - loss: 7.5380e-04 - val_loss: 6.2267e-04\n",
      "Epoch 57/60\n",
      "8976/8976 [==============================] - 2s 264us/sample - loss: 7.5381e-04 - val_loss: 6.2185e-04\n",
      "Epoch 58/60\n",
      "8976/8976 [==============================] - 2s 266us/sample - loss: 7.5379e-04 - val_loss: 6.2126e-04\n",
      "Epoch 59/60\n",
      "8976/8976 [==============================] - 2s 269us/sample - loss: 7.5382e-04 - val_loss: 6.2211e-04\n",
      "Epoch 60/60\n",
      "8976/8976 [==============================] - 2s 274us/sample - loss: 7.5379e-04 - val_loss: 6.1947e-04\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    Xtr_seq, Xtr_seq,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon = model.predict(Xte_seq, batch_size=max(1, BATCH_SIZE), verbose=0)\n",
    "recon_flat = recon.reshape(recon.shape[0], recon.shape[2])    \n",
    "true_flat = Xte_seq.reshape(Xte_seq.shape[0], Xte_seq.shape[2])\n",
    "sq_err_each = (recon_flat - true_flat) ** 2\n",
    "s_t = sq_err_each.mean(axis=1).astype(np.float32)  \n",
    "\n",
    "L = likelihood_scores_from_s(s_t, W=W_LONG, Wp=W_SHORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = pd.to_datetime(X_test_scaled.index)[:len(L)]\n",
    "try:\n",
    "    timestamps = timestamps.tz_localize(None)\n",
    "except Exception:\n",
    "    pass\n",
    "timestamps = timestamps.round(ROUND_RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-timestamp counts:\n",
      "  TP=8  TN=25451  FP=163  FN=865  total=26487\n"
     ]
    }
   ],
   "source": [
    "gt_df = gt_in_test.copy()\n",
    "gt_df[\"anomaly_start\"] = pd.to_datetime(gt_df[\"anomaly_start\"]).dt.tz_localize(None)\n",
    "gt_df[\"anomaly_end\"]   = pd.to_datetime(gt_df[\"anomaly_end\"]).dt.tz_localize(None)\n",
    "gt_df[\"start_rounded\"] = gt_df[\"anomaly_start\"].dt.round(ROUND_RES)\n",
    "gt_df[\"end_rounded\"]   = gt_df[\"anomaly_end\"].dt.round(ROUND_RES)\n",
    "\n",
    "t0, t1 = timestamps[0], timestamps[-1]\n",
    "gt_df[\"start_clipped\"] = gt_df[\"start_rounded\"].clip(lower=t0, upper=t1)\n",
    "gt_df[\"end_clipped\"] = gt_df[\"end_rounded\"].clip(lower=t0, upper=t1)\n",
    "gt_df = gt_df[gt_df[\"start_clipped\"] <= gt_df[\"end_clipped\"]].reset_index(drop=True)\n",
    "\n",
    "gt_series = pd.Series(0, index=timestamps)\n",
    "for _, r in gt_df.iterrows():\n",
    "    s, e = r[\"start_clipped\"], r[\"end_clipped\"]\n",
    "    mask = (gt_series.index >= s) & (gt_series.index <= e)\n",
    "    if mask.any():\n",
    "        gt_series.loc[mask] = 1\n",
    "\n",
    "scores_arr = np.asarray(L)\n",
    "scores_df = pd.DataFrame({\"timestamp\": timestamps, \"anomaly_score\": scores_arr}).set_index(\"timestamp\")\n",
    "pred_series = (scores_df[\"anomaly_score\"] >= PAPER_THR).astype(int)\n",
    "pred_mask_bool = pred_series.values.astype(bool)\n",
    "\n",
    "TP = int(((pred_series == 1) & (gt_series == 1)).sum())\n",
    "FP = int(((pred_series == 1) & (gt_series == 0)).sum())\n",
    "TN = int(((pred_series == 0) & (gt_series == 0)).sum())\n",
    "FN = int(((pred_series == 0) & (gt_series == 1)).sum())\n",
    "print(\"\\nPer-timestamp counts:\")\n",
    "print(f\"  TP={TP}  TN={TN}  FP={FP}  FN={FN}  total={len(pred_series)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Window-based normalized NAB scores:\n",
      "  Standard: 17.00 | event-counts: {'TP': 4, 'FN': 14, 'FP': 121}\n",
      "  Low-FN  : 18.74 | event-counts: {'TP': 4, 'FN': 14, 'FP': 121}\n"
     ]
    }
   ],
   "source": [
    "norm_standard, counts_standard = nab_score_from_mask(pred_mask_bool, gt_df, timestamps,\n",
    "                                                    profile=\"standard\",\n",
    "                                                    event_gap_minutes=EVENT_GAP_MINUTES,\n",
    "                                                    pct_prob=PCT_PROB)\n",
    "norm_lowfn, counts_lowfn = nab_score_from_mask(pred_mask_bool, gt_df, timestamps,\n",
    "                                               profile=\"low_fn\",\n",
    "                                               event_gap_minutes=EVENT_GAP_MINUTES,\n",
    "                                               pct_prob=PCT_PROB)\n",
    "print(\"\\nWindow-based normalized NAB scores:\")\n",
    "print(f\"  Standard: {norm_standard:.2f} | event-counts: {counts_standard}\")\n",
    "print(f\"  Low-FN  : {norm_lowfn:.2f} | event-counts: {counts_lowfn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved per-timestamp preds to: userData/pivoted_paper_baseline/per_timestamp_preds_thr_09973.csv\n",
      "Saved event alerts CSV to: userData/pivoted_paper_baseline/baseline_alerts_thr_09973.csv\n",
      "Saved diagnostic plots.\n"
     ]
    }
   ],
   "source": [
    "per_ts_path = os.path.join(OUT_DIR, f\"per_timestamp_preds_thr_{str(PAPER_THR).replace('.', '')}.csv\")\n",
    "scores_df.assign(pred=pred_series.values, gt=gt_series.values).to_csv(per_ts_path)\n",
    "events = mask_to_event_times(pred_mask_bool, scores_df.index, event_gap_minutes=EVENT_GAP_MINUTES, pct_prob=PCT_PROB)\n",
    "ev_df = pd.DataFrame({\"alert_time\": pd.to_datetime(events)})\n",
    "ev_path = os.path.join(OUT_DIR, f\"baseline_alerts_thr_{str(PAPER_THR).replace('.', '')}.csv\")\n",
    "ev_df.to_csv(ev_path, index=False)\n",
    "print(f\"\\nSaved per-timestamp preds to: {per_ts_path}\")\n",
    "print(f\"Saved event alerts CSV to: {ev_path}\")\n",
    "\n",
    "try:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    counts = df[\"sum_5xx_count\"].fillna(0).values\n",
    "    if len(counts) > 0:\n",
    "        min_x = max(1, counts.min() + 1)\n",
    "        max_x = counts.max() + 1\n",
    "        bins = np.logspace(max(0, np.log10(min_x) - 0.1), np.log10(max_x) + 0.1, 80)\n",
    "        plt.hist(counts + 1e-9, bins=bins, histtype=\"step\", linewidth=1.5)\n",
    "        plt.xscale(\"log\"); plt.yscale(\"log\")\n",
    "    plt.title(\"Distribution of 5XX counts (log-log)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, \"fig2_5xx_distribution.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(14, 3))\n",
    "    plt.plot(scores_df.index, scores_df[\"anomaly_score\"], label=\"Lt (anomaly score)\")\n",
    "    plt.axhline(PAPER_THR, color=\"red\", linestyle=\"--\", label=f\"thr={PAPER_THR}\")\n",
    "    for _, r in gt_df.iterrows():\n",
    "        s, e = r[\"start_clipped\"], r[\"end_clipped\"]\n",
    "        plt.axvspan(s, e, color=\"purple\", alpha=0.12)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, \"diag_lt_with_gt.png\"), dpi=150)\n",
    "    plt.close()\n",
    "    print(\"Saved diagnostic plots.\")\n",
    "except Exception as e:\n",
    "    print(\"Plotting error (ignored):\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
