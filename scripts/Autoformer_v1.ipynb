{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, time, random, re, types\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "PIVOTED_FILE = 'pivoted_data.parquet'             \n",
    "ANOMALY_WINDOWS_FILE = 'anomaly_windows.csv'    \n",
    "OUT_DIR = 'userData/pivoted_paper_autoformer_tf'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "WIN_SIZE = 100         \n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 20\n",
    "LR = 1e-4\n",
    "PAPER_THR = 0.9973      \n",
    "\n",
    "\n",
    "ROUND_RES = \"5T\"\n",
    "EVENT_GAP_MINUTES = 15\n",
    "PCT_PROB = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_parquet_smart(path, time_col='interval_start'):\n",
    "    df = pd.read_parquet(path) if str(path).endswith('.parquet') else pd.read_csv(path)\n",
    "    if time_col in df.columns and not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n",
    "        df = df.sort_values(time_col).set_index(time_col)\n",
    "   \n",
    "    if isinstance(df.index, pd.DatetimeIndex) and df.index.year.max() < 1990:\n",
    "        ns = df.index.view('int64')\n",
    "        maxv = float(ns.max())\n",
    "        if maxv < 1e12:\n",
    "            scale = 1_000_000_000\n",
    "        elif maxv < 1e15:\n",
    "            scale = 1_000_000\n",
    "        elif maxv < 1e18:\n",
    "            scale = 1_000\n",
    "        else:\n",
    "            scale = 1\n",
    "        df = df.copy()\n",
    "        df.index = pd.to_datetime(ns * scale, unit='ns')\n",
    "    return df\n",
    "\n",
    "def add_time_features(index):\n",
    "    if not isinstance(index, pd.DatetimeIndex):\n",
    "        index = pd.to_datetime(index, errors='coerce')\n",
    "    ts_seconds = index.asi8 // 10**9\n",
    "    day = 24 * 3600\n",
    "    week = 7 * day\n",
    "    return pd.DataFrame({\n",
    "        'sin_day': np.sin(2 * np.pi * (ts_seconds % day) / day),\n",
    "        'cos_day': np.cos(2 * np.pi * (ts_seconds % day) / day),\n",
    "        'sin_week': np.sin(2 * np.pi * (ts_seconds % week) / week),\n",
    "        'cos_week': np.cos(2 * np.pi * (ts_seconds % week) / week),\n",
    "    }, index=index).astype(np.float32)\n",
    "\n",
    "def make_sequences(df_values, L=1):\n",
    "    arr = df_values.values\n",
    "    if L == 1:\n",
    "        return arr[:, None, :]\n",
    "    n = len(arr) - L + 1\n",
    "    return np.stack([arr[i:i+L] for i in range(n)], axis=0)\n",
    "\n",
    "\n",
    "NAB_PROFILES = {\n",
    "    'standard': {'TP': 1.0, 'FN': -1.0, 'FP': -0.11, 'TN': 0.0},\n",
    "    'low_fn': {'TP': 1.0, 'FN': -2.0, 'FP': -0.11, 'TN': 0.0}\n",
    "}\n",
    "\n",
    "def scaled_sigmoid(x, lo, hi, slope=5.0):\n",
    "    v = 1.0 / (1.0 + np.exp(-slope * x))\n",
    "    return lo + (hi - lo) * v\n",
    "\n",
    "def tp_score(time_hit, start, end, tp_weight):\n",
    "    L = (end - start).total_seconds()\n",
    "    if L <= 0:\n",
    "        return tp_weight\n",
    "    r = (time_hit - start).total_seconds() / L\n",
    "    r = np.clip(r, 0, 1)\n",
    "    x = 1 - 2 * r\n",
    "    return scaled_sigmoid(x, 0, tp_weight)\n",
    "\n",
    "def fp_penalty(time_hit, windows, fp_weight):\n",
    "    if not windows:\n",
    "        return fp_weight\n",
    "    dists = [abs((time_hit - s).total_seconds()) for (s, e) in windows]\n",
    "    d = min(dists)\n",
    "    mean_len = np.mean([(e - s).total_seconds() for (s, e) in windows]) if windows else 60\n",
    "    z = d / max(mean_len, 1)\n",
    "    return min(0, scaled_sigmoid(z, fp_weight, 0))\n",
    "\n",
    "def mask_to_event_times(pred_mask, timestamps, event_gap_minutes=15, pct_prob=0.0):\n",
    "    cutoff = timestamps[0] + (timestamps[-1] - timestamps[0]) * pct_prob\n",
    "    pm = np.asarray(pred_mask) & (timestamps >= cutoff)\n",
    "    edges = np.where(pm & (np.roll(pm, 1) == 0))[0]\n",
    "    if len(edges) == 0:\n",
    "        return []\n",
    "    events = [timestamps[edges[0]]]\n",
    "    gap = pd.Timedelta(minutes=event_gap_minutes)\n",
    "    for idx in edges[1:]:\n",
    "        if timestamps[idx] - events[-1] >= gap:\n",
    "            events.append(timestamps[idx])\n",
    "    return events\n",
    "\n",
    "def nab_score_from_mask(pred_mask, gt_windows_df, timestamps, profile='standard', event_gap_minutes=15, pct_prob=0.0):\n",
    "    prof = NAB_PROFILES[profile]\n",
    "    TPw, FNw, FPw = prof['TP'], prof['FN'], prof['FP']\n",
    "    wins = [(pd.Timestamp(r['anomaly_start']), pd.Timestamp(r['anomaly_end'])) for _, r in gt_windows_df.iterrows()]\n",
    "    events = mask_to_event_times(pred_mask, timestamps, event_gap_minutes, pct_prob)\n",
    "    score = 0.0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    used = [False] * len(wins)\n",
    "    for ev in events:\n",
    "        hit = -1\n",
    "        for i, (s, e) in enumerate(wins):\n",
    "            if (not used[i]) and (s <= ev <= e):\n",
    "                hit = i\n",
    "                break\n",
    "        if hit >= 0:\n",
    "            score += tp_score(ev, wins[hit][0], wins[hit][1], TPw)\n",
    "            used[hit] = True\n",
    "            TP += 1\n",
    "        else:\n",
    "            score += fp_penalty(ev, wins, FPw)\n",
    "            FP += 1\n",
    "    FN = sum(not u for u in used)\n",
    "    score += FNw * FN\n",
    "    perfect = sum(tp_score(s, s, e, TPw) for (s, e) in wins)\n",
    "    null = FNw * len(wins)\n",
    "    if np.isclose(perfect, null):\n",
    "        normalized = 0.0\n",
    "    else:\n",
    "        normalized = 100.0 * (score - null) / (perfect - null)\n",
    "    return normalized, {'TP': TP, 'FN': FN, 'FP': FP}\n",
    "\n",
    "def likelihood_scores_from_s(err, W=30, Wp=2):\n",
    "    T = len(err)\n",
    "    L = np.zeros(T)\n",
    "    eps = 1e-8\n",
    "    for t in range(T):\n",
    "        a = max(0, t - W + 1)\n",
    "        b = t + 1\n",
    "        a2 = max(0, t - Wp + 1)\n",
    "        long_window = err[a:b]\n",
    "        short_window = err[a2:b]\n",
    "        mu = long_window.mean()\n",
    "        std = long_window.std(ddof=1) if (b - a) > 1 else eps\n",
    "        mu_s = short_window.mean()\n",
    "        z = (mu_s - mu) / (std + eps)\n",
    "        L[t] = norm.cdf(z)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Loading data from:', PIVOTED_FILE)\n",
    "df = read_parquet_smart(PIVOTED_FILE, time_col='interval_start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_patterns = [r'(5xx|_5\\d\\d_)', r'_count$']\n",
    "compiled = [re.compile(p, re.I) for p in include_patterns]\n",
    "feature_cols = [c for c in df.columns if all(p.search(c) for p in compiled)]\n",
    "print('Selected feature columns:', len(feature_cols))\n",
    "if len(feature_cols) == 0:\n",
    "    raise ValueError('No features matched the selection patterns.')\n",
    "\n",
    "five_xx_cols = [c for c in df.columns if re.search(r'5xx|_5\\d\\d_', c, re.I) and c.endswith('_count')]\n",
    "df['sum_5xx_count'] = df[five_xx_cols].fillna(0).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feature_cols].fillna(0).astype(np.float32)\n",
    "X = pd.concat([X, add_time_features(X.index)], axis=1)\n",
    "\n",
    "TRAIN_START, TRAIN_END = pd.Timestamp('2024-01-26'), pd.Timestamp('2024-02-29 23:59:59')\n",
    "TEST_START, TEST_END = pd.Timestamp('2024-03-01'), pd.Timestamp('2024-05-31 23:59:59')\n",
    "\n",
    "train_df = X[(X.index >= TRAIN_START) & (X.index <= TRAIN_END)].copy()\n",
    "test_df  = X[(X.index >= TEST_START) & (X.index <= TEST_END)].copy()\n",
    "print('Train rows:', len(train_df), 'Test rows:', len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gt_all = pd.read_csv(ANOMALY_WINDOWS_FILE)\n",
    "s_cols = [c for c in gt_all.columns if 'start' in c.lower()]\n",
    "e_cols = [c for c in gt_all.columns if 'end' in c.lower()]\n",
    "if not s_cols or not e_cols:\n",
    "    raise ValueError('Could not find start/end columns in anomaly_windows.csv')\n",
    "gt_df_raw = pd.DataFrame({\n",
    "    'anomaly_start': pd.to_datetime(gt_all[s_cols[0]].astype(str).str.replace(r'([+-]\\d{2}:?\\d{2}|Z)$','', regex=True), errors='coerce'),\n",
    "    'anomaly_end':   pd.to_datetime(gt_all[e_cols[0]].astype(str).str.replace(r'([+-]\\d{2}:?\\d{2}|Z)$','', regex=True), errors='coerce')\n",
    "}).dropna().reset_index(drop=True)\n",
    "\n",
    "gt_in_test = gt_df_raw[(gt_df_raw['anomaly_end'] >= test_df.index[0]) & (gt_df_raw['anomaly_start'] <= test_df.index[-1])].reset_index(drop=True)\n",
    "print('GT windows total:', len(gt_df_raw), 'GT inside test:', len(gt_in_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train_anomalies = True\n",
    "if mask_train_anomalies:\n",
    "    train_idx = train_df.index\n",
    "    keep_mask = np.ones(len(train_idx), dtype=bool)\n",
    "    for s, e in gt_df_raw.itertuples(index=False):\n",
    "        keep_mask &= ~((train_idx >= s) & (train_idx <= e))\n",
    "    removed = len(train_df) - keep_mask.sum()\n",
    "    train_df = train_df.loc[keep_mask]\n",
    "    print('Rows removed from training due to masking GT windows:', removed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_df)\n",
    "train_scaled = pd.DataFrame(scaler.transform(train_df), index=train_df.index, columns=train_df.columns)\n",
    "test_scaled = pd.DataFrame(scaler.transform(test_df), index=test_df.index, columns=test_df.columns)\n",
    "\n",
    "def sliding_windows_from_df(df_scaled, win_size, stride=1):\n",
    "    arr = df_scaled.values\n",
    "    n = len(arr)\n",
    "    if n < win_size:\n",
    "        return np.zeros((0, win_size, arr.shape[1])), np.zeros((0,)), []\n",
    "    Xw = []\n",
    "    ts = []\n",
    "    for i in range(0, n - win_size + 1, stride):\n",
    "        Xw.append(arr[i:i+win_size])\n",
    "        ts.append(df_scaled.index[i+win_size-1])  # align to last timestamp\n",
    "    return np.stack(Xw, axis=0), np.zeros((len(Xw),), dtype=int), ts\n",
    "\n",
    "win_size = WIN_SIZE\n",
    "Xtr_w, ytr_w, tr_ts = sliding_windows_from_df(train_scaled, win_size)\n",
    "Xte_w, yte_w, te_ts = sliding_windows_from_df(test_scaled, win_size)\n",
    "\n",
    "print('Train windows:', Xtr_w.shape, 'Test windows:', Xte_w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "except AttributeError:\n",
    "    try:\n",
    "        AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    except AttributeError:\n",
    "        AUTOTUNE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xtr = Xtr_w.astype('float32')\n",
    "Xte = Xte_w.astype('float32')\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((Xtr, Xtr))\n",
    "train_ds = train_ds.shuffle(2048, seed=SEED).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((Xte, Xte))\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "print(\"tf version:\", tf.__version__, \"using AUTOTUNE =\", AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = np.zeros((max_len, d_model), dtype=np.float32)\n",
    "        position = np.arange(max_len)[:, None]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        self.pe = tf.constant(pe[None, :, :], dtype=tf.float32)\n",
    "    def call(self, x):\n",
    "        L = tf.shape(x)[1]\n",
    "        return tf.cast(self.pe[:, :L, :], x.dtype)\n",
    "\n",
    "class TokenEmbedding(layers.Layer):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super().__init__()\n",
    "        self.conv = layers.Conv1D(filters=d_model, kernel_size=3, padding='same', activation=None)\n",
    "    def call(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class TemporalEmbedding(layers.Layer):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super().__init__()\n",
    "        self.proj = layers.Dense(d_model)\n",
    "    def call(self, x_mark):\n",
    "        if x_mark is None:\n",
    "            return 0.0\n",
    "        return self.proj(x_mark)\n",
    "\n",
    "class DataEmbedding_wo_pos(layers.Layer):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.value_embedding = TokenEmbedding(c_in, d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type, freq=freq) if True else None\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "    def call(self, x, x_mark=None, training=False):\n",
    "        if x_mark is None:\n",
    "            x = self.value_embedding(x)\n",
    "        else:\n",
    "            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n",
    "        return self.dropout(x, training=training)\n",
    "\n",
    "class DataEmbedding(layers.Layer):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.value_embedding = TokenEmbedding(c_in, d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "    def call(self, x, x_mark=None, training=False):\n",
    "        if x_mark is None:\n",
    "            emb = self.value_embedding(x) + self.position_embedding(x)\n",
    "        else:\n",
    "            emb = self.value_embedding(x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
    "        return self.dropout(emb, training=training)\n",
    "\n",
    "class MovingAvg(layers.Layer):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "\n",
    "    def call(self, x):\n",
    "       \n",
    "        left = x[:, :1, :]                         \n",
    "        right = x[:, -1:, :]                      \n",
    "        pad_left = tf.tile(left, [1, self.padding, 1])  \n",
    "        pad_right = tf.tile(right, [1, self.padding, 1]) \n",
    "        x_p = tf.concat([pad_left, x, pad_right], axis=1) \n",
    "\n",
    "        kernel = tf.ones((self.kernel_size, 1, 1), dtype=x.dtype) / tf.cast(self.kernel_size, x.dtype)\n",
    "        B = tf.shape(x_p)[0]; Lp = tf.shape(x_p)[1]; C = tf.shape(x_p)[2]\n",
    "\n",
    "       \n",
    "        x_resh = tf.reshape(x_p, (B * C, Lp, 1))\n",
    "        kernel_tf = tf.reshape(kernel, (self.kernel_size, 1, 1))\n",
    "        out = tf.nn.conv1d(x_resh, filters=kernel_tf, stride=1, padding='VALID') \n",
    "        out = tf.reshape(out, (B, C, -1))  # (B, C, L)\n",
    "        out = tf.transpose(out, perm=[0, 2, 1])  # (B, L, C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SeriesDecomp(layers.Layer):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.mavg = MovingAvg(kernel_size)\n",
    "    def call(self, x):\n",
    "        trend = self.mavg(x)\n",
    "        resid = x - trend\n",
    "        return resid, trend\n",
    "\n",
    "class AutoCorrelationInner(layers.Layer):\n",
    "    def __init__(self, mask_flag=True, factor=1, attention_dropout=0.1, output_attention=False):\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = layers.Dropout(attention_dropout)\n",
    "    def time_delay_agg_training(self, values, corr):\n",
    "        \"\"\"\n",
    "        Graph-friendly training-time time-delay aggregation.\n",
    "        values: (B, H, C, L)\n",
    "        corr:   (B, H, C, L)\n",
    "        \"\"\"\n",
    "        \n",
    "        length = tf.shape(values)[-1]\n",
    "        \n",
    "        top_k = tf.maximum(1, tf.cast(tf.math.floor(self.factor * tf.math.log(tf.cast(length, tf.float32))), tf.int32))\n",
    "      \n",
    "        mean_value = tf.reduce_mean(tf.reduce_mean(corr, axis=1), axis=1)  \n",
    "        \n",
    "        vals, idx = tf.math.top_k(mean_value, k=top_k)  \n",
    "        weights = tf.nn.softmax(vals, axis=-1)           \n",
    "\n",
    "        B = tf.shape(values)[0]\n",
    "        delays_agg = tf.zeros_like(values)\n",
    "\n",
    "        \n",
    "        i0 = tf.constant(0, dtype=tf.int32)\n",
    "\n",
    "        def cond(i, delays_agg):\n",
    "            return tf.less(i, top_k)\n",
    "\n",
    "        def body(i, delays_agg):\n",
    "            \n",
    "            shifts = tf.gather(idx, i, axis=1)  \n",
    "          \n",
    "            def roll_one(args):\n",
    "                v, s = args\n",
    "                return tf.roll(v, shift=-s, axis=-1)\n",
    "            rolled = tf.map_fn(lambda z: roll_one(z), (values, shifts), dtype=values.dtype)\n",
    "            w = tf.reshape(tf.gather(weights, i, axis=1), (B, 1, 1, 1))\n",
    "            delays_agg = delays_agg + rolled * w\n",
    "            return i + 1, delays_agg\n",
    "\n",
    "        _, delays_agg = tf.while_loop(cond, body, loop_vars=[i0, delays_agg], parallel_iterations=1)\n",
    "        return delays_agg\n",
    "\n",
    "    def time_delay_agg_inference(self, values, corr):\n",
    "        \n",
    "        length = tf.shape(values)[-1]\n",
    "        top_k = tf.maximum(1, tf.cast(tf.math.floor(self.factor * tf.math.log(tf.cast(length, tf.float32))), tf.int32))\n",
    "        mean_value = tf.reduce_mean(tf.reduce_mean(corr, axis=1), axis=1)  \n",
    "        vals, delay = tf.math.top_k(mean_value, k=top_k)  \n",
    "        weights = tf.nn.softmax(vals, axis=-1)            \n",
    "\n",
    "        B = tf.shape(values)[0]\n",
    "        delays_agg = tf.zeros_like(values)\n",
    "        i0 = tf.constant(0, dtype=tf.int32)\n",
    "\n",
    "        def cond(i, delays_agg):\n",
    "            return tf.less(i, top_k)\n",
    "\n",
    "        def body(i, delays_agg):\n",
    "            d = tf.gather(delay, i, axis=1)  \n",
    "            def roll_one(args):\n",
    "                v, s = args\n",
    "                return tf.roll(v, shift=-s, axis=-1)\n",
    "            gathered = tf.map_fn(lambda z: roll_one(z), (values, d), dtype=values.dtype)\n",
    "            w = tf.reshape(tf.gather(weights, i, axis=1), (B, 1, 1, 1))\n",
    "            delays_agg = delays_agg + gathered * w\n",
    "            return i + 1, delays_agg\n",
    "\n",
    "        _, delays_agg = tf.while_loop(cond, body, loop_vars=[i0, delays_agg], parallel_iterations=1)\n",
    "        return delays_agg\n",
    "\n",
    "    def call(self, queries, keys, values, training=True):\n",
    "        \n",
    "        qf = tf.signal.rfft(queries)\n",
    "        kf = tf.signal.rfft(keys)\n",
    "        res = qf * tf.math.conj(kf)\n",
    "\n",
    "        length = tf.shape(queries)[-1]\n",
    "        corr = tf.signal.irfft(res, fft_length=[length])\n",
    "\n",
    "        if training:\n",
    "            V = self.time_delay_agg_training(values, corr)\n",
    "        else:\n",
    "            V = self.time_delay_agg_inference(values, corr)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return V, corr\n",
    "        else:\n",
    "            return V, None\n",
    "\n",
    "\n",
    "class AutoCorrelationLayerK(layers.Layer):\n",
    "   \n",
    "    def __init__(self, correlation_module, d_model, n_heads, d_keys=None, d_values=None):\n",
    "        super().__init__()\n",
    "        self.inner = correlation_module\n",
    "        self.n_heads = n_heads\n",
    "       \n",
    "        self.d_keys = d_keys or (d_model // n_heads)\n",
    "        self.d_values = d_values or (d_model // n_heads)\n",
    "        \n",
    "        self.q_proj = layers.Dense(self.d_keys * n_heads, use_bias=True)\n",
    "        self.k_proj = layers.Dense(self.d_keys * n_heads, use_bias=True)\n",
    "        self.v_proj = layers.Dense(self.d_values * n_heads, use_bias=True)\n",
    "        \n",
    "        self.out_dim_in = int(self.n_heads * self.d_values)\n",
    "        self.out_dim_out = int(d_model)\n",
    "        \n",
    "        self.out_proj_w = self.add_weight(\n",
    "            name='out_proj_w',\n",
    "            shape=(self.out_dim_in, self.out_dim_out),\n",
    "            initializer=keras.initializers.glorot_uniform(),\n",
    "            trainable=True,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "       \n",
    "        self.out_proj_b = self.add_weight(\n",
    "            name='out_proj_b',\n",
    "            shape=(self.out_dim_out,),\n",
    "            initializer='zeros',\n",
    "            trainable=True,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def call(self, queries, keys, values, attn_mask=None, training=False):\n",
    "      \n",
    "        B = tf.shape(queries)[0]; L = tf.shape(queries)[1]\n",
    "        S = tf.shape(keys)[1]\n",
    "        H = self.n_heads\n",
    "\n",
    "       \n",
    "        q = self.q_proj(queries)  \n",
    "        k = self.k_proj(keys)    \n",
    "        v = self.v_proj(values)  \n",
    "\n",
    "        \n",
    "        q = tf.reshape(q, (B, L, H, -1))\n",
    "        k = tf.reshape(k, (B, S, H, -1))\n",
    "        v = tf.reshape(v, (B, S, H, -1))\n",
    "\n",
    "        \n",
    "        q_inner = tf.transpose(q, perm=[0,2,3,1]) \n",
    "        k_inner = tf.transpose(k, perm=[0,2,3,1])  \n",
    "        v_inner = tf.transpose(v, perm=[0,2,3,1]) \n",
    "\n",
    "        out, attn = self.inner(q_inner, k_inner, v_inner, training=training)\n",
    "       \n",
    "        out = tf.transpose(out, perm=[0,3,1,2])\n",
    "     \n",
    "        out = tf.reshape(out, (B, L, -1))  \n",
    "\n",
    "       \n",
    "        projected = tf.tensordot(out, self.out_proj_w, axes=[[2], [0]]) \n",
    "        projected = projected + self.out_proj_b \n",
    "        return projected, attn\n",
    "\n",
    "\n",
    "class MyLayerNorm(layers.Layer):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.ln = layers.LayerNormalization(axis=-1, epsilon=1e-6)\n",
    "    def call(self, x):\n",
    "        x_hat = self.ln(x)\n",
    "        bias = tf.reduce_mean(x_hat, axis=1, keepdims=True)\n",
    "        return x_hat - bias\n",
    "\n",
    "\n",
    "class EncoderLayerK(layers.Layer):\n",
    "    def __init__(self, attention_layer, d_model, d_ff=None, moving_avg=25, dropout=0.1, activation='relu'):\n",
    "        super().__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention_layer\n",
    "        \n",
    "        self.conv1 = layers.Conv1D(filters=d_ff, kernel_size=1, use_bias=False)\n",
    "        self.conv2 = layers.Conv1D(filters=d_model, kernel_size=1, use_bias=False)\n",
    "        self.decomp1 = SeriesDecomp(moving_avg)\n",
    "        self.decomp2 = SeriesDecomp(moving_avg)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.activation = tf.nn.relu if activation == 'relu' else tf.nn.gelu\n",
    "\n",
    "    def call(self, x, attn_mask=None, training=False):\n",
    "        # x: (B, L, D)\n",
    "        new_x, attn = self.attention(x, x, x, attn_mask, training=training)\n",
    "        x = x + self.dropout(new_x, training=training)\n",
    "        x, _ = self.decomp1(x)\n",
    "        y = x\n",
    "       \n",
    "        y = self.dropout(self.activation(self.conv1(y)), training=training) \n",
    "        y = self.dropout(self.conv2(y), training=training)                  \n",
    "        res, _ = self.decomp2(x + y)\n",
    "        return res, attn\n",
    "\n",
    "\n",
    "class EncoderK(layers.Layer):\n",
    "    def __init__(self, attn_layers, norm_layer=None):\n",
    "        super().__init__()\n",
    "        self.attn_layers = attn_layers\n",
    "        self.norm = norm_layer\n",
    "    def call(self, x, attn_mask=None, training=False):\n",
    "        attns = []\n",
    "        for layer in self.attn_layers:\n",
    "            x, attn = layer(x, attn_mask=attn_mask, training=training)\n",
    "            attns.append(attn)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x, attns\n",
    "\n",
    "class DecoderLayerK(layers.Layer):\n",
    "    def __init__(self, self_attention, cross_attention, d_model, c_out, d_ff=None, moving_avg=25, dropout=0.1, activation='relu'):\n",
    "        super().__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        \n",
    "        self.conv1 = layers.Conv1D(filters=d_ff, kernel_size=1, use_bias=False)\n",
    "        self.conv2 = layers.Conv1D(filters=d_model, kernel_size=1, use_bias=False)\n",
    "        self.decomp1 = SeriesDecomp(moving_avg)\n",
    "        self.decomp2 = SeriesDecomp(moving_avg)\n",
    "        self.decomp3 = SeriesDecomp(moving_avg)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        \n",
    "        self.proj_conv = layers.Conv1D(filters=c_out, kernel_size=3, padding='same')\n",
    "        self.activation = tf.nn.relu if activation == 'relu' else tf.nn.gelu\n",
    "\n",
    "    def call(self, x, cross, x_mask=None, cross_mask=None, training=False):\n",
    "       \n",
    "        x = x + self.dropout(self.self_attention(x, x, x, x_mask, training=training)[0], training=training)\n",
    "        x, trend1 = self.decomp1(x)\n",
    "        \n",
    "        x = x + self.dropout(self.cross_attention(x, cross, cross, cross_mask, training=training)[0], training=training)\n",
    "        x, trend2 = self.decomp2(x)\n",
    "        y = x\n",
    "        \n",
    "        y = self.dropout(self.activation(self.conv1(y)), training=training)  \n",
    "        y = self.dropout(self.conv2(y), training=training)                  \n",
    "        x, trend3 = self.decomp3(x + y)\n",
    "        residual_trend = trend1 + trend2 + trend3  \n",
    "        \n",
    "        residual_trend_proj = self.proj_conv(residual_trend) \n",
    "        return x, residual_trend_proj\n",
    "\n",
    "class DecoderK(layers.Layer):\n",
    "    def __init__(self, layers_list, norm_layer=None, projection=None):\n",
    "        super().__init__()\n",
    "        self.layers = layers_list\n",
    "        self.norm = norm_layer\n",
    "        self.projection = projection\n",
    "    def call(self, x, cross, x_mask=None, cross_mask=None, trend=None, training=False):\n",
    "        for layer in self.layers:\n",
    "            x, residual_trend = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask, training=training)\n",
    "            trend = trend + residual_trend\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        if self.projection is not None:\n",
    "            x = self.projection(x)\n",
    "        return x, trend\n",
    "\n",
    "class AutoformerKerasModel(keras.Model):\n",
    "    def __init__(self, configs):\n",
    "        super().__init__()\n",
    "        self.task_name = configs.task_name\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.label_len = configs.label_len\n",
    "        self.pred_len = configs.pred_len\n",
    "        kernel_size = getattr(configs, 'moving_avg', 25)\n",
    "        self.decomp = SeriesDecomp(kernel_size)\n",
    "        self.enc_embedding = DataEmbedding_wo_pos(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n",
    "        if self.task_name in ('long_term_forecast', 'short_term_forecast'):\n",
    "            self.dec_embedding = DataEmbedding_wo_pos(configs.dec_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n",
    "        enc_layers = []\n",
    "        for _ in range(configs.e_layers):\n",
    "            autocorr = AutoCorrelationInner(mask_flag=False, factor=configs.factor, attention_dropout=configs.dropout, output_attention=False)\n",
    "            ac_layer = AutoCorrelationLayerK(autocorr, configs.d_model, configs.n_heads)\n",
    "            enc_layers.append(EncoderLayerK(ac_layer, configs.d_model, configs.d_ff, moving_avg=kernel_size, dropout=configs.dropout, activation=configs.activation))\n",
    "        self.encoder = EncoderK(enc_layers, norm_layer=MyLayerNorm(configs.d_model))\n",
    "        if self.task_name in ('long_term_forecast', 'short_term_forecast'):\n",
    "            dec_layers = []\n",
    "            for _ in range(configs.d_layers):\n",
    "                self_att = AutoCorrelationLayerK(AutoCorrelationInner(mask_flag=True, factor=configs.factor, attention_dropout=configs.dropout, output_attention=False), configs.d_model, configs.n_heads)\n",
    "                cross_att = AutoCorrelationLayerK(AutoCorrelationInner(mask_flag=False, factor=configs.factor, attention_dropout=configs.dropout, output_attention=False), configs.d_model, configs.n_heads)\n",
    "                dec_layers.append(DecoderLayerK(self_att, cross_att, configs.d_model, configs.c_out, configs.d_ff, moving_avg=kernel_size, dropout=configs.dropout, activation=configs.activation))\n",
    "            proj = layers.Dense(configs.c_out)\n",
    "            self.decoder = DecoderK(dec_layers, norm_layer=MyLayerNorm(configs.d_model), projection=proj)\n",
    "        if self.task_name in ('imputation', 'anomaly_detection'):\n",
    "            self.projection = layers.Dense(configs.c_out)\n",
    "        if self.task_name == 'classification':\n",
    "            self.act = tf.nn.gelu\n",
    "            self.dropout = layers.Dropout(configs.dropout)\n",
    "            self.projection = layers.Dense(configs.num_class)\n",
    "\n",
    "    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec, training=False):\n",
    "        mean = tf.reduce_mean(x_enc, axis=1, keepdims=True)\n",
    "        mean = tf.tile(mean, [1, self.pred_len, 1])\n",
    "        zeros = tf.zeros([tf.shape(x_enc)[0], self.pred_len, tf.shape(x_enc)[2]], dtype=x_enc.dtype)\n",
    "        seasonal_init, trend_init = self.decomp(x_enc)\n",
    "        trend_init = tf.concat([trend_init[:, -self.label_len:, :], mean], axis=1)\n",
    "        seasonal_init = tf.concat([seasonal_init[:, -self.label_len:, :], zeros], axis=1)\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc, training=training)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=None, training=training)\n",
    "        dec_out = self.dec_embedding(seasonal_init, x_mark_dec, training=training)\n",
    "        seasonal_part, trend_part = self.decoder(dec_out, enc_out, x_mask=None, cross_mask=None, trend=trend_init, training=training)\n",
    "        dec_out = seasonal_part + trend_part\n",
    "        return dec_out\n",
    "\n",
    "    def imputation(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None, training=False):\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc, training=training)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=None, training=training)\n",
    "        dec_out = self.projection(enc_out)\n",
    "        return dec_out\n",
    "\n",
    "    def anomaly_detection(self, x_enc, training=False):\n",
    "        enc_out = self.enc_embedding(x_enc, None, training=training)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=None, training=training)\n",
    "        dec_out = self.projection(enc_out)\n",
    "        return dec_out\n",
    "\n",
    "    def classification(self, x_enc, x_mark_enc, training=False):\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc, training=training)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=None, training=training)\n",
    "        output = self.act(enc_out)\n",
    "        output = self.dropout(output, training=training)\n",
    "        if x_mark_enc is not None:\n",
    "            output = output * tf.expand_dims(x_mark_enc, -1)\n",
    "        output = tf.reshape(output, (tf.shape(output)[0], -1))\n",
    "        output = self.projection(output)\n",
    "        return output\n",
    "\n",
    "    def call(self, x_enc, x_mark_enc=None, x_dec=None, x_mark_dec=None, mask=None, training=False):\n",
    "        if self.task_name in ('long_term_forecast', 'short_term_forecast'):\n",
    "            dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec, training=training)\n",
    "            return dec_out[:, -self.pred_len:, :]\n",
    "        if self.task_name == 'imputation':\n",
    "            return self.imputation(x_enc, x_mark_enc, x_dec, x_mark_dec, mask=mask, training=training)\n",
    "        if self.task_name == 'anomaly_detection':\n",
    "            return self.anomaly_detection(x_enc, training=training)\n",
    "        if self.task_name == 'classification':\n",
    "            return self.classification(x_enc, x_mark_enc, training=training)\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg = types.SimpleNamespace()\n",
    "cfg.task_name = 'anomaly_detection'\n",
    "cfg.seq_len = WIN_SIZE\n",
    "cfg.label_len = WIN_SIZE // 2\n",
    "cfg.pred_len = 0\n",
    "cfg.moving_avg = 25\n",
    "cfg.enc_in = Xtr_w.shape[-1]\n",
    "cfg.dec_in = Xtr_w.shape[-1]\n",
    "cfg.d_model = 64\n",
    "cfg.n_heads = 4\n",
    "cfg.d_ff = 256\n",
    "cfg.e_layers = 3\n",
    "cfg.d_layers = 2\n",
    "cfg.c_out = Xtr_w.shape[-1]\n",
    "cfg.factor = 1\n",
    "cfg.dropout = 0.05\n",
    "cfg.embed = 'fixed'\n",
    "cfg.freq = 'h'\n",
    "cfg.activation = 'relu'\n",
    "cfg.num_class = 2\n",
    "model = AutoformerKerasModel(cfg)\n",
    "model.build(input_shape=(None, WIN_SIZE, cfg.enc_in))  \n",
    "_ = model(tf.zeros((1, WIN_SIZE, cfg.enc_in)), training=False) \n",
    "model.compile(optimizer=keras.optimizers.Adam(LR), loss='mse')\n",
    "best_path = os.path.join(OUT_DIR, 'autoformer_ae_checkpoint.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(best_path, save_weights_only=True, save_best_only=False)\n",
    "print('Starting training...')\n",
    "history = model.fit(train_ds, epochs=NUM_EPOCHS, validation_data=None, callbacks=[checkpoint_cb], verbose=1)\n",
    "print('Training finished. checkpoint saved to', best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_weights(best_path)\n",
    "\n",
    "all_rec_err = []\n",
    "for batch_x, _ in test_ds:\n",
    "    batch_x = tf.cast(batch_x, tf.float32)\n",
    "    output = model(batch_x, training=False)  \n",
    "    loss = tf.reduce_mean(tf.square(output - batch_x), axis=[1,2]) \n",
    "    all_rec_err.append(loss.numpy())\n",
    "all_rec_err = np.concatenate(all_rec_err, axis=0).reshape(-1)\n",
    "print('Per-window reconstruction errors:', all_rec_err.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "timestamps = pd.to_datetime(te_ts)\n",
    "minlen = min(len(all_rec_err), len(timestamps))\n",
    "all_rec_err = all_rec_err[:minlen]\n",
    "timestamps = timestamps[:minlen]\n",
    "\n",
    "W_LONG = 30\n",
    "W_SHORT = 2\n",
    "L = likelihood_scores_from_s(all_rec_err, W=W_LONG, Wp=W_SHORT)\n",
    "scores_df = pd.DataFrame({'timestamp': timestamps, 'anomaly_score': L}).set_index('timestamp')\n",
    "\n",
    "pred_series = (scores_df['anomaly_score'] >= PAPER_THR).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gt_df = gt_in_test.copy()\n",
    "gt_df['anomaly_start'] = pd.to_datetime(gt_df['anomaly_start']).dt.tz_localize(None)\n",
    "gt_df['anomaly_end'] = pd.to_datetime(gt_df['anomaly_end']).dt.tz_localize(None)\n",
    "gt_df['start_rounded'] = gt_df['anomaly_start'].dt.round(ROUND_RES)\n",
    "gt_df['end_rounded'] = gt_df['anomaly_end'].dt.round(ROUND_RES)\n",
    "t0, t1 = timestamps[0], timestamps[-1]\n",
    "gt_df['start_clipped'] = gt_df['start_rounded'].clip(lower=t0, upper=t1)\n",
    "gt_df['end_clipped'] = gt_df['end_rounded'].clip(lower=t0, upper=t1)\n",
    "gt_df = gt_df[gt_df['start_clipped'] <= gt_df['end_clipped']].reset_index(drop=True)\n",
    "\n",
    "gt_series = pd.Series(0, index=timestamps)\n",
    "for _, r in gt_df.iterrows():\n",
    "    s, e = r['start_clipped'], r['end_clipped']\n",
    "    mask = (gt_series.index >= s) & (gt_series.index <= e)\n",
    "    if mask.any():\n",
    "        gt_series.loc[mask] = 1\n",
    "\n",
    "TP = int(((pred_series == 1) & (gt_series == 1)).sum())\n",
    "FP = int(((pred_series == 1) & (gt_series == 0)).sum())\n",
    "TN = int(((pred_series == 0) & (gt_series == 0)).sum())\n",
    "FN = int(((pred_series == 0) & (gt_series == 1)).sum())\n",
    "print('\\nPer-timestamp counts:')\n",
    "print(f'  TP={TP}  TN={TN}  FP={FP}  FN={FN}  total={len(pred_series)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_standard, counts_standard = nab_score_from_mask(pred_series.astype(bool), gt_df, timestamps, profile='standard')\n",
    "norm_lowfn, counts_lowfn = nab_score_from_mask(pred_series.astype(bool), gt_df, timestamps, profile='low_fn')\n",
    "print('\\nWindow-based normalized NAB scores:')\n",
    "print(f'  Standard: {norm_standard:.2f} | event-counts: {counts_standard}')\n",
    "print(f'  Low-FN  : {norm_lowfn:.2f} | event-counts: {counts_lowfn}')\n",
    "\n",
    "TP_ts = int(((pred_series == 1) & (gt_series == 1)).sum())\n",
    "FP_ts = int(((pred_series == 1) & (gt_series == 0)).sum())\n",
    "TN_ts = int(((pred_series == 0) & (gt_series == 0)).sum())\n",
    "FN_ts = int(((pred_series == 0) & (gt_series == 1)).sum())\n",
    "total_ts = len(pred_series)\n",
    "\n",
    "prec_ts = (TP_ts / (TP_ts + FP_ts)) if (TP_ts + FP_ts) > 0 else 0.0\n",
    "rec_ts  = (TP_ts / (TP_ts + FN_ts)) if (TP_ts + FN_ts) > 0 else 0.0\n",
    "f1_ts   = (2 * prec_ts * rec_ts / (prec_ts + rec_ts)) if (prec_ts + rec_ts) > 0 else 0.0\n",
    "\n",
    "print(\"\\nPer-timestamp counts:\")\n",
    "print(f\"  TP={TP_ts}  TN={TN_ts}  FP={FP_ts}  FN={FN_ts}  total={total_ts}\")\n",
    "print(\"Per-timestamp metrics (percentage out of 100):\")\n",
    "print(f\"  Precision: {100.0 * prec_ts:.2f}%\")\n",
    "print(f\"  Recall   : {100.0 * rec_ts:.2f}%\")\n",
    "print(f\"  F1-score : {100.0 * f1_ts:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ts_metrics = {\n",
    "    \"kind\": \"per_timestamp\",\n",
    "    \"TP\": TP_ts, \"FP\": FP_ts, \"TN\": TN_ts, \"FN\": FN_ts,\n",
    "    \"precision_pct\": 100.0 * prec_ts, \"recall_pct\": 100.0 * rec_ts, \"f1_pct\": 100.0 * f1_ts,\n",
    "    \"total\": total_ts\n",
    "}\n",
    "\n",
    "\n",
    "per_ts_path = os.path.join(OUT_DIR, f'per_timestamp_preds_thr_{str(PAPER_THR).replace(\".\",\"\")}.csv')\n",
    "scores_df.assign(pred=pred_series.values, gt=gt_series.values).to_csv(per_ts_path)\n",
    "events = mask_to_event_times(pred_series.astype(bool), scores_df.index, event_gap_minutes=EVENT_GAP_MINUTES, pct_prob=0.0)\n",
    "ev_df = pd.DataFrame({'alert_time': pd.to_datetime(events)})\n",
    "ev_path = os.path.join(OUT_DIR, f'autoformer_alerts_thr_{str(PAPER_THR).replace(\".\",\"\")}.csv')\n",
    "ev_df.to_csv(ev_path, index=False)\n",
    "print('\\nSaved per-timestamp preds to:', per_ts_path)\n",
    "print('Saved event alerts CSV to:', ev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s_arr = np.asarray(all_rec_err).copy()    \n",
    "L_raw_arr = np.asarray(L).copy()          \n",
    "seq_timestamps = pd.to_datetime(timestamps)\n",
    "try:\n",
    "    seq_timestamps = seq_timestamps.tz_localize(None)\n",
    "except Exception:\n",
    "    pass\n",
    "minlen = min(len(s_arr), len(L_raw_arr), len(seq_timestamps))\n",
    "s_arr = s_arr[:minlen]\n",
    "L_raw_arr = L_raw_arr[:minlen]\n",
    "seq_timestamps = seq_timestamps[:minlen]\n",
    "\n",
    "mse_series = pd.Series(s_arr, index=seq_timestamps)\n",
    "lt_series  = pd.Series(L_raw_arr, index=seq_timestamps)\n",
    "\n",
    "\n",
    "if ('df' in globals()) and ('sum_5xx_count' in df.columns):\n",
    "    sum_5xx = df['sum_5xx_count'].loc[seq_timestamps[0]:seq_timestamps[-1]].fillna(0)\n",
    "    sum_norm = (sum_5xx - sum_5xx.min()) / (sum_5xx.max() - sum_5xx.min() + 1e-12)\n",
    "else:\n",
    "    sum_norm = pd.Series([], dtype=float)\n",
    "\n",
    "\n",
    "def ema(x, alpha):\n",
    "    out = np.empty_like(x, dtype=float)\n",
    "    acc = 0.0\n",
    "    for i, v in enumerate(x):\n",
    "        acc = alpha * v + (1 - alpha) * acc if i else v\n",
    "        out[i] = acc\n",
    "    return out\n",
    "\n",
    "def shaped_alerts(Ls, thr, cd, k_of_m=(2,3)):\n",
    "    k, m = k_of_m\n",
    "    above = (Ls >= thr)\n",
    "    n = len(above)\n",
    "    persisted = np.zeros_like(above, dtype=bool)\n",
    "    for i in range(n):\n",
    "        persisted[i] = (above[max(0, i-m+1):i+1].sum() >= k)\n",
    "    trig = np.zeros_like(above, dtype=bool)\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        if persisted[i]:\n",
    "            trig[i] = True\n",
    "            i += cd\n",
    "        else:\n",
    "            i += 1\n",
    "    return trig\n",
    "\n",
    "\n",
    "VAL_START, VAL_END = pd.Timestamp(\"2024-03-01\"), pd.Timestamp(\"2024-03-15 23:59:59\")\n",
    "tidx_arr = np.array(seq_timestamps)\n",
    "val_mask = (tidx_arr >= np.datetime64(VAL_START)) & (tidx_arr <= np.datetime64(VAL_END))\n",
    "t_val = tidx_arr[val_mask]\n",
    "gt_val = gt_in_test.copy()\n",
    "if len(t_val)>0:\n",
    "    gt_val = gt_val[(gt_val['anomaly_end'] >= pd.Timestamp(t_val[0])) & (gt_val['anomaly_start'] <= pd.Timestamp(t_val[-1]))].reset_index(drop=True)\n",
    "else:\n",
    "    gt_val = gt_val.iloc[0:0]\n",
    "\n",
    "best = {\"score\": -1}\n",
    "for alpha in [0.2, 0.3, 0.5]:\n",
    "    Ls_try = ema(L_raw_arr, alpha)\n",
    "    L_val = Ls_try[val_mask] if len(val_mask)>0 else np.array([])\n",
    "    for k_of_m in [(2,3), (3,5)]:\n",
    "        for cd in [12, 24, 36, 72]:\n",
    "            for thr in np.linspace(0.992, 0.9999, 20):\n",
    "                if len(L_val)==0:\n",
    "                    continue\n",
    "                pred_val = shaped_alerts(L_val, thr, cd, k_of_m)\n",
    "                s, _ = nab_score_from_mask(pred_val, gt_val, pd.to_datetime(t_val), profile='standard', event_gap_minutes=cd, pct_prob=PCT_PROB)\n",
    "                if s > best[\"score\"]:\n",
    "                    best = {\"score\": s, \"alpha\": alpha, \"k_of_m\": k_of_m, \"cd\": cd, \"thr\": thr}\n",
    "if best[\"score\"] < 0:\n",
    "    best = {\"alpha\":0.3, \"k_of_m\":(2,3), \"cd\":24, \"thr\":PAPER_THR}\n",
    "\n",
    "L_s = ema(L_raw_arr, best[\"alpha\"])\n",
    "pred_full_mask = shaped_alerts(L_s, best[\"thr\"], best[\"cd\"], best[\"k_of_m\"])\n",
    "pred_series_shaped = pd.Series(pred_full_mask.astype(int), index=seq_timestamps)\n",
    "events_shaped = mask_to_event_times(pred_full_mask, seq_timestamps, event_gap_minutes=best[\"cd\"], pct_prob=PCT_PROB)\n",
    "scores_df_smoothed = pd.DataFrame({\"timestamp\": seq_timestamps, \"anomaly_score\": L_s}).set_index(\"timestamp\")\n",
    "\n",
    "\n",
    "fig2_path = os.path.join(OUT_DIR, 'fig2_5xx_distribution_autoformer_gru_style.png')\n",
    "counts = df['sum_5xx_count'].fillna(0).values if ('df' in globals() and 'sum_5xx_count' in df.columns) else np.array([])\n",
    "plt.figure(figsize=(6,4))\n",
    "if len(counts)>0:\n",
    "    min_x = max(1, counts.min()+1)\n",
    "    max_x = counts.max()+1\n",
    "    bins = np.logspace(max(0, np.log10(min_x)-0.1), np.log10(max_x)+0.1, 80)\n",
    "    plt.hist(counts + 1e-9, bins=bins, histtype='step', linewidth=1.5)\n",
    "    plt.xscale('log'); plt.yscale('log')\n",
    "    mean_v = counts.mean(); median_v = np.median(counts)\n",
    "    try:\n",
    "        mode_v = stats.mode(counts)[0][0]\n",
    "    except Exception:\n",
    "        mode_v = np.nan\n",
    "    plt.gca().text(0.95, 0.95, f\"Mean: {mean_v:.2f}\\nMedian: {median_v:.2f}\\nMode: {mode_v:.2f}\",\n",
    "                   transform=plt.gca().transAxes, va='top', ha='right', bbox=dict(boxstyle='round', fc='white', alpha=0.8))\n",
    "plt.title('Fig.2 — Distribution of request counts associated with 5XX errors')\n",
    "plt.tight_layout(); plt.savefig(fig2_path, dpi=150); plt.show()\n",
    "print(\"Saved Fig.2 ->\", fig2_path)\n",
    "\n",
    "\n",
    "fig3_path = os.path.join(OUT_DIR, 'fig3_sum5xx_norm_with_alerts_autoformer.png')\n",
    "plt.figure(figsize=(12,3)); ax=plt.gca()\n",
    "if not sum_norm.empty:\n",
    "    ax.plot(sum_norm.index, sum_norm.values, label='Sum of 5XX Count (Normalized)', linewidth=1)\n",
    "\n",
    "gt_to_draw = gt_in_test if 'gt_in_test' in globals() else (gt_df if 'gt_df' in globals() else None)\n",
    "if gt_to_draw is not None and not gt_to_draw.empty:\n",
    "    for _, r in gt_to_draw.iterrows():\n",
    "        s = pd.to_datetime(r['anomaly_start']).tz_localize(None) if hasattr(r['anomaly_start'],'tz') else pd.to_datetime(r['anomaly_start'])\n",
    "        e = pd.to_datetime(r['anomaly_end']).tz_localize(None) if hasattr(r['anomaly_end'],'tz') else pd.to_datetime(r['anomaly_end'])\n",
    "        ax.axvspan(s,e,color='purple',alpha=0.12)\n",
    "\n",
    "if len(events_shaped):\n",
    "    ev_idx = pd.to_datetime(events_shaped)\n",
    "    try:\n",
    "        interp_vals = np.interp(ev_idx.astype('int64'), sum_norm.index.astype('int64'), sum_norm.values)\n",
    "        plt.scatter(ev_idx, interp_vals, marker='x', color='red', s=40, label='Predicted Anomalies')\n",
    "    except Exception:\n",
    "        plt.scatter(ev_idx, [0]*len(ev_idx), marker='x', color='red', s=40, label='Predicted Anomalies')\n",
    "plt.title('Fig.3 — Sum of 5XX Count (Normalized) with GT and Predicted Anomalies')\n",
    "plt.legend(loc='upper right'); plt.tight_layout(); plt.savefig(fig3_path, dpi=150); plt.show()\n",
    "print(\"Saved Fig.3 ->\", fig3_path)\n",
    "\n",
    "\n",
    "fig4_path = os.path.join(OUT_DIR, 'fig4_mar12_15_autoformer.png')\n",
    "start4 = pd.Timestamp(\"2024-03-12\"); end4 = pd.Timestamp(\"2024-03-15\")\n",
    "ix_sum = sum_norm.loc[start4:end4] if not sum_norm.empty else pd.Series([], dtype=float)\n",
    "ix_mse = mse_series.loc[start4:end4] if not mse_series.empty else pd.Series([], dtype=float)\n",
    "ix_lt  = lt_series.loc[start4:end4]  if not lt_series.empty else pd.Series([], dtype=float)\n",
    "plt.figure(figsize=(12,4)); ax=plt.gca()\n",
    "if not ix_sum.empty: ax.plot(ix_sum.index, ix_sum.values, label='5XX Count (Normalized)', linewidth=1)\n",
    "if not ix_mse.empty: ax.plot(ix_mse.index, (ix_mse - ix_mse.min())/(ix_mse.max()-ix_mse.min()+1e-12), label='Reconstruction Error (Normalized)', linewidth=1)\n",
    "if not ix_lt.empty:  ax.plot(ix_lt.index, ix_lt.values, label='Anomaly Likelihood (Lt)', linewidth=1)\n",
    "ev_win = [e for e in events_shaped if (e >= start4) and (e <= end4)]\n",
    "if len(ev_win) and not ix_lt.empty:\n",
    "    vals = np.interp(pd.to_datetime(ev_win).astype('int64'), ix_lt.index.astype('int64'), ix_lt.values)\n",
    "    plt.scatter(ev_win, vals, marker='x', color='red', s=50, label='Predicted Alerts')\n",
    "\n",
    "if gt_to_draw is not None and not gt_to_draw.empty:\n",
    "    ix_gt = gt_to_draw[(gt_to_draw['anomaly_start'] >= start4) & (gt_to_draw['anomaly_start'] <= end4)]\n",
    "    for _, r in ix_gt.iterrows():\n",
    "        s = pd.to_datetime(r['anomaly_start']).tz_localize(None) if hasattr(r['anomaly_start'],'tz') else pd.to_datetime(r['anomaly_start'])\n",
    "        e = pd.to_datetime(r['anomaly_end']).tz_localize(None) if hasattr(r['anomaly_end'],'tz') else pd.to_datetime(r['anomaly_end'])\n",
    "        ax.axvspan(s,e,color='purple',alpha=0.12)\n",
    "plt.title('Fig.4 — Mar 12-15, 2024 example: 5XX, Recon Error, Likelihood'); plt.legend(); plt.tight_layout(); plt.savefig(fig4_path, dpi=150); plt.show()\n",
    "print(\"Saved Fig.4 ->\", fig4_path)\n",
    "\n",
    "\n",
    "fig5_path = os.path.join(OUT_DIR, 'fig5_apr15_18_autoformer.png')\n",
    "start5 = pd.Timestamp(\"2024-04-15\"); end5 = pd.Timestamp(\"2024-04-18\")\n",
    "ix_sum = sum_norm.loc[start5:end5] if not sum_norm.empty else pd.Series([], dtype=float)\n",
    "ix_mse = mse_series.loc[start5:end5] if not mse_series.empty else pd.Series([], dtype=float)\n",
    "ix_lt  = lt_series.loc[start5:end5]  if not lt_series.empty else pd.Series([], dtype=float)\n",
    "plt.figure(figsize=(12,4)); ax=plt.gca()\n",
    "if not ix_sum.empty: ax.plot(ix_sum.index, ix_sum.values, label='5XX Count (Normalized)', linewidth=1)\n",
    "if not ix_mse.empty: ax.plot(ix_mse.index, (ix_mse - ix_mse.min())/(ix_mse.max()-ix_mse.min()+1e-12), label='Reconstruction Error (Normalized)', linewidth=1)\n",
    "if not ix_lt.empty:  ax.plot(ix_lt.index, ix_lt.values, label='Anomaly Likelihood (Lt)', linewidth=1)\n",
    "ev_win = [e for e in events_shaped if (e >= start5) and (e <= end5)]\n",
    "if len(ev_win) and not ix_lt.empty:\n",
    "    vals = np.interp(pd.to_datetime(ev_win).astype('int64'), ix_lt.index.astype('int64'), ix_lt.values)\n",
    "    plt.scatter(ev_win, vals, marker='x', color='red', s=50, label='Predicted Alerts')\n",
    "if gt_to_draw is not None and not gt_to_draw.empty:\n",
    "    ix_gt = gt_to_draw[(gt_to_draw['anomaly_start'] >= start5) & (gt_to_draw['anomaly_start'] <= end5)]\n",
    "    for _, r in ix_gt.iterrows():\n",
    "        s = pd.to_datetime(r['anomaly_start']).tz_localize(None) if hasattr(r['anomaly_start'],'tz') else pd.to_datetime(r['anomaly_start'])\n",
    "        e = pd.to_datetime(r['anomaly_end']).tz_localize(None) if hasattr(r['anomaly_end'],'tz') else pd.to_datetime(r['anomaly_end'])\n",
    "        ax.axvspan(s,e,color='purple',alpha=0.12)\n",
    "plt.title('Fig.5 — Apr 15-18, 2024 example: detected abnormal behavior'); plt.legend(); plt.tight_layout(); plt.savefig(fig5_path, dpi=150); plt.show()\n",
    "print(\"Saved Fig.5 ->\", fig5_path)\n",
    "\n",
    "\n",
    "try:\n",
    "    plt.figure(figsize=(14,3))\n",
    "    plt.plot(mse_series, label='mean recon MSE (s_t)')\n",
    "    if gt_to_draw is not None and not gt_to_draw.empty:\n",
    "        for _, r in gt_to_draw.iterrows():\n",
    "            s = pd.to_datetime(r['anomaly_start']).tz_localize(None) if hasattr(r['anomaly_start'],'tz') else pd.to_datetime(r['anomaly_start'])\n",
    "            e = pd.to_datetime(r['anomaly_end']).tz_localize(None) if hasattr(r['anomaly_end'],'tz') else pd.to_datetime(r['anomaly_end'])\n",
    "            plt.axvspan(s,e,color='red', alpha=0.12)\n",
    "    plt.yscale('symlog', linthresh=1e-6)\n",
    "    plt.title('Mean reconstruction MSE (s_t) with GT windows'); plt.legend(); plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('MSE plot error (ignored):', e)\n",
    "\n",
    "try:\n",
    "    plt.figure(figsize=(14,3))\n",
    "    plt.plot(scores_df_smoothed.index, scores_df_smoothed['anomaly_score'], label='Lt (anomaly score)')\n",
    "    plt.axhline(best[\"thr\"], color='red', linestyle='--', label=f'thr={best[\"thr\"]:.6f}')\n",
    "    if gt_to_draw is not None and not gt_to_draw.empty:\n",
    "        for _, r in gt_to_draw.iterrows():\n",
    "            s = pd.to_datetime(r['anomaly_start']).tz_localize(None) if hasattr(r['anomaly_start'],'tz') else pd.to_datetime(r['anomaly_start'])\n",
    "            e = pd.to_datetime(r['anomaly_end']).tz_localize(None) if hasattr(r['anomaly_end'],'tz') else pd.to_datetime(r['anomaly_end'])\n",
    "            plt.axvspan(s, e, color='red', alpha=0.12)\n",
    "    ev_dt = pd.to_datetime(events_shaped)\n",
    "    if len(ev_dt):\n",
    "        plt.scatter(ev_dt, [best[\"thr\"]]*len(ev_dt), marker='x', s=40, label='alerts', zorder=5)\n",
    "    plt.title('Anomaly likelihood Lt with GT windows & predicted alerts')\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "except Exception as e:\n",
    "    print('Score plot error (ignored):', e)\n",
    "\n",
    "print(\"\\nAll figures saved to\", OUT_DIR)\n",
    "print('All finished. Outputs saved in:', OUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
